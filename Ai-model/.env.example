# ==================== MODEL SELECTION ====================
# Choose which AI model to use (options: "multitask", "llama-1b", "qwen-3b")
AI_MODEL_TYPE=llama-1b

# ==================== MODEL PATHS ====================
# Paths to model directories (default: ./model/<ModelName>)
MULTITASK_JUDGE_MODEL_PATH=./model/Multi_model
LLAMA_1B_MODEL_PATH=./model/Llama-1B
QWEN_3B_MODEL_PATH=./model/Qwen-3B

# ==================== LLAMA-1B SETTINGS ====================
# 4-bit quantization for Llama (saves VRAM, recommended for GPUs with <16GB)
LLAMA_USE_4BIT=true
# Maximum context length for Llama
LLAMA_MAX_SEQ_LENGTH=4096
# Device allocation ("auto", "cuda", "cpu")
LLAMA_DEVICE_MAP=auto

# ==================== QWEN-3B SETTINGS ====================
# 4-bit quantization for Qwen (saves VRAM)
QWEN_USE_4BIT=true
# Maximum context length for Qwen
QWEN_MAX_SEQ_LENGTH=2048
# Device allocation ("auto", "cuda", "cpu")
QWEN_DEVICE_MAP=auto

# ==================== API SERVER SETTINGS ====================
HOST=0.0.0.0
PORT=8000

# ==================== CORS SETTINGS ====================
CORS_ORIGINS=*

# ==================== LOGGING ====================
LOG_LEVEL=INFO
