# Ch·∫°y 2 Services Ri√™ng Bi·ªát

## ∆Øu ƒëi·ªÉm

‚úÖ **Nh·∫π h∆°n**: M·ªói service ch·ªâ d√πng ~3GB RAM thay v√¨ ~6-8GB  
‚úÖ **·ªîn ƒë·ªãnh**: Kh√¥ng b·ªã crash do thi·∫øu RAM  
‚úÖ **ƒê·ªôc l·∫≠p**: Scale t·ª´ng service theo nhu c·∫ßu  
‚úÖ **Linh ho·∫°t**: C√≥ th·ªÉ ch·∫°y tr√™n 2 m√°y kh√°c nhau  

---

## C√°ch ch·∫°y

### Terminal 1: GenQ Service (Port 8000)
```powershell
cd D:\Code\NCKH\AI-Interview\Ai-model
py main_genq_only.py
```

**Endpoints:**
- `http://localhost:8000/api/v1/initial-question` - L·∫•y c√¢u h·ªèi ƒë·∫ßu ti√™n
- `http://localhost:8000/api/v1/generate-question` - Generate c√¢u h·ªèi
- `http://localhost:8000/health` - Health check

**RAM Usage:** ~3GB

---

### Terminal 2: Judge Service (Port 8001)
```powershell
cd D:\Code\NCKH\AI-Interview\Ai-model
py main_judge_only.py
```

**Endpoints:**
- `http://localhost:8001/api/v1/evaluate-answer` - ƒê√°nh gi√° c√¢u tr·∫£ l·ªùi
- `http://localhost:8001/health` - Health check

**RAM Usage:** ~3GB

---

## Test v·ªõi Postman

### 1. Generate Question (GenQ Service - Port 8000)

**POST** `http://localhost:8000/api/v1/generate-question`

```json
{
  "cv_text": "Experienced Java developer with 5 years in Spring Boot",
  "jd_text": "Building microservices with Spring Boot",
  "role": "Java Backend Developer",
  "level": "Senior",
  "skills": ["Spring Boot", "Microservices"],
  "max_tokens": 64,
  "temperature": 0.7
}
```

---

### 2. Evaluate Answer (Judge Service - Port 8001)

**POST** `http://localhost:8001/api/v1/evaluate-answer`

```json
{
  "question": "Explain dependency injection in Spring Boot",
  "answer": "DI is a design pattern where Spring automatically injects dependencies using @Autowired. It promotes loose coupling and easier testing.",
  "role": "Java Backend Developer",
  "level": "Mid-level",
  "competency": "Spring Boot",
  "skills": ["Spring Boot", "Dependency Injection"]
}
```

---

## Full Interview Flow

### Step 1: Get Initial Question (GenQ)
**POST** `http://localhost:8000/api/v1/initial-question`
```json
{
  "role": "Python Developer",
  "level": "Mid-level",
  "skills": ["FastAPI", "PostgreSQL"]
}
```

### Step 2: Generate Follow-up (GenQ)
**POST** `http://localhost:8000/api/v1/generate-question`
```json
{
  "role": "Python Developer",
  "level": "Mid-level",
  "skills": ["FastAPI"],
  "previous_question": "Tell me about yourself",
  "previous_answer": "I'm a Python developer with 3 years experience"
}
```

### Step 3: Evaluate Answer (Judge)
**POST** `http://localhost:8001/api/v1/evaluate-answer`
```json
{
  "question": "What are performance considerations in FastAPI?",
  "answer": "FastAPI is fast due to async/await. Use async DB drivers, caching, connection pooling.",
  "role": "Python Developer",
  "level": "Mid-level"
}
```

---

## So s√°nh v·ªõi Single Service

| Aspect | Single Service | Separate Services |
|--------|---------------|-------------------|
| **RAM Usage** | ~6-8GB (c·∫£ 2 models) | ~3GB m·ªói service |
| **Startup Time** | 2-3 ph√∫t | 1-2 ph√∫t m·ªói service |
| **Stability** | D·ªÖ crash n·∫øu RAM th·∫•p | ·ªîn ƒë·ªãnh h∆°n |
| **Scalability** | Kh√≥ scale | D·ªÖ scale ƒë·ªôc l·∫≠p |
| **Deployment** | 1 container | 2 containers |
| **Development** | ƒê∆°n gi·∫£n h∆°n | Ph·ª©c t·∫°p h∆°n 1 ch√∫t |

---

## Docker Compose (Optional)

T·∫°o file `docker-compose-separate.yml`:

```yaml
version: '3.8'

services:
  genq-service:
    build: ./Ai-model
    command: python main_genq_only.py
    ports:
      - "8000:8000"
    environment:
      - MODEL_PATH=/app/model/Merge
    volumes:
      - ./Ai-model/model/Merge:/app/model/Merge
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 3G

  judge-service:
    build: ./Ai-model
    command: python main_judge_only.py
    ports:
      - "8001:8001"
    environment:
      - JUDGE_MODEL_PATH=/app/model/Judge_merge
    volumes:
      - ./Ai-model/model/Judge_merge:/app/model/Judge_merge
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 3G
```

Ch·∫°y:
```bash
docker-compose -f docker-compose-separate.yml up
```

---

## Monitoring

### Check Service Status

```powershell
# GenQ Service
curl http://localhost:8000/health

# Judge Service
curl http://localhost:8001/health
```

### Check RAM Usage

```powershell
# Get process memory
Get-Process python | Select-Object ProcessName, @{Name="Memory(GB)";Expression={$_.WorkingSet / 1GB}}
```

---

## Production Deployment

### Nginx Reverse Proxy

```nginx
upstream genq_service {
    server localhost:8000;
}

upstream judge_service {
    server localhost:8001;
}

server {
    listen 80;
    server_name api.ai-interview.com;

    location /api/v1/generate-question {
        proxy_pass http://genq_service;
    }

    location /api/v1/initial-question {
        proxy_pass http://genq_service;
    }

    location /api/v1/evaluate-answer {
        proxy_pass http://judge_service;
    }
}
```

---

## Troubleshooting

### GenQ Service kh√¥ng start
- Check port 8000 c√≥ b·ªã chi·∫øm kh√¥ng: `netstat -ano | findstr :8000`
- Check RAM available: C·∫ßn √≠t nh·∫•t 4GB free

### Judge Service kh√¥ng start
- Check port 8001 c√≥ b·ªã chi·∫øm kh√¥ng: `netstat -ano | findstr :8001`
- Check RAM available: C·∫ßn √≠t nh·∫•t 4GB free

### Response ch·∫≠m
- First request s·∫Ω ch·∫≠m do load model
- Requests sau nhanh h∆°n (~2-5s)

---

## Next Steps

1. ‚úÖ Test c·∫£ 2 services ri√™ng bi·ªát
2. ‚úÖ Verify RAM usage ·ªïn ƒë·ªãnh
3. ‚úÖ Measure response times
4. üîÑ Deploy to production v·ªõi load balancer
5. üîÑ Add monitoring (Prometheus/Grafana)
